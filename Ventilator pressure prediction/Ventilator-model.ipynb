{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d44fd017",
   "metadata": {
    "papermill": {
     "duration": 0.043883,
     "end_time": "2022-01-04T05:58:18.883252",
     "exception": false,
     "start_time": "2022-01-04T05:58:18.839369",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3308601",
   "metadata": {
    "papermill": {
     "duration": 0.043195,
     "end_time": "2022-01-04T05:58:18.968390",
     "exception": false,
     "start_time": "2022-01-04T05:58:18.925195",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> ## 1.1 Import Libraries\n",
    "\n",
    "First, we import python modules required for data wrangling, exploratory data analysis, model building and data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37f8fb32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T05:58:19.057486Z",
     "iopub.status.busy": "2022-01-04T05:58:19.056327Z",
     "iopub.status.idle": "2022-01-04T05:58:28.173422Z",
     "shell.execute_reply": "2022-01-04T05:58:28.172482Z",
     "shell.execute_reply.started": "2022-01-04T05:24:14.089044Z"
    },
    "papermill": {
     "duration": 9.161923,
     "end_time": "2022-01-04T05:58:28.173606",
     "exception": false,
     "start_time": "2022-01-04T05:58:19.011683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data manipulation and visualization libraries\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes._axes as axes\n",
    "from matplotlib.figure import Figure\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "from warnings import filterwarnings\n",
    "\n",
    "#data prepocessing and classic ML libraries\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, LabelEncoder, RobustScaler, normalize \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import KFold, RepeatedKFold, StratifiedKFold \n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet, LogisticRegression, Ridge\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier \n",
    "from sklearn.ensemble import ExtraTreesRegressor, ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor, AdaBoostClassifier \n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier  \n",
    "\n",
    "#for imbalanced classification\n",
    "from imblearn.ensemble import BalancedBaggingClassifier, BalancedRandomForestClassifier\n",
    "from imblearn.ensemble import EasyEnsembleClassifier, RUSBoostClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "#neural network libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.math import reduce_prod\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Dense, Activation\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.metrics import AUC\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "np.random.seed(123)\n",
    "filterwarnings('ignore')\n",
    "pd.set_option('display.float_format', lambda num:'%1.3f'%num)\n",
    "pd.set_option('display.max_columns', 99)\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bizarre-passport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.18.5 in c:\\users\\chunh\\anaconda3\\lib\\site-packages (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "#!pip install xgboost\n",
    "#!pip uninstall pycocotools\n",
    "#!pip install pycocotools-windows\n",
    "np.version.version\n",
    "!pip install numpy==1.18.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056ee9c2",
   "metadata": {
    "papermill": {
     "duration": 0.040133,
     "end_time": "2022-01-04T05:58:28.256401",
     "exception": false,
     "start_time": "2022-01-04T05:58:28.216268",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> ## 1.2. Load Datasets\n",
    "\n",
    "To get started, the provided datasets are loaded as pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6498f1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T05:58:28.343631Z",
     "iopub.status.busy": "2022-01-04T05:58:28.343006Z",
     "iopub.status.idle": "2022-01-04T05:58:38.154426Z",
     "shell.execute_reply": "2022-01-04T05:58:38.155058Z",
     "shell.execute_reply.started": "2022-01-04T05:24:14.109548Z"
    },
    "papermill": {
     "duration": 9.858525,
     "end_time": "2022-01-04T05:58:38.155247",
     "exception": false,
     "start_time": "2022-01-04T05:58:28.296722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breath_id</th>\n",
       "      <th>time_step</th>\n",
       "      <th>u_in</th>\n",
       "      <th>pressure</th>\n",
       "      <th>R_20</th>\n",
       "      <th>R_5</th>\n",
       "      <th>R_50</th>\n",
       "      <th>C_10</th>\n",
       "      <th>C_20</th>\n",
       "      <th>C_50</th>\n",
       "      <th>u_out_0</th>\n",
       "      <th>u_out_1</th>\n",
       "      <th>R_C_20_10</th>\n",
       "      <th>R_C_20_20</th>\n",
       "      <th>R_C_20_50</th>\n",
       "      <th>R_C_50_10</th>\n",
       "      <th>R_C_50_20</th>\n",
       "      <th>R_C_50_50</th>\n",
       "      <th>R_C_5_10</th>\n",
       "      <th>R_C_5_20</th>\n",
       "      <th>R_C_5_50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5.837</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.184</td>\n",
       "      <td>5.908</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.225</td>\n",
       "      <td>7.876</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.228</td>\n",
       "      <td>11.743</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.254</td>\n",
       "      <td>12.235</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.080</td>\n",
       "      <td>15.047</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.079</td>\n",
       "      <td>15.117</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>2</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.082</td>\n",
       "      <td>14.696</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.073</td>\n",
       "      <td>15.820</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.074</td>\n",
       "      <td>15.750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  breath_id  time_step  u_in  pressure  R_20  R_5  R_50  C_10  C_20  \\\n",
       "0     1          1      0.000 0.001     5.837     1    0     0     0     0   \n",
       "1     2          1      0.034 0.184     5.908     1    0     0     0     0   \n",
       "2     3          1      0.068 0.225     7.876     1    0     0     0     0   \n",
       "3     4          1      0.102 0.228    11.743     1    0     0     0     0   \n",
       "4     5          1      0.136 0.254    12.235     1    0     0     0     0   \n",
       "..  ...        ...        ...   ...       ...   ...  ...   ...   ...   ...   \n",
       "95   96          2      0.510 0.080    15.047     1    0     0     0     1   \n",
       "96   97          2      0.544 0.079    15.117     1    0     0     0     1   \n",
       "97   98          2      0.578 0.082    14.696     1    0     0     0     1   \n",
       "98   99          2      0.612 0.073    15.820     1    0     0     0     1   \n",
       "99  100          2      0.646 0.074    15.750     1    0     0     0     1   \n",
       "\n",
       "    C_50  u_out_0  u_out_1  R_C_20_10  R_C_20_20  R_C_20_50  R_C_50_10  \\\n",
       "0      1        1        0          0          0          1          0   \n",
       "1      1        1        0          0          0          1          0   \n",
       "2      1        1        0          0          0          1          0   \n",
       "3      1        1        0          0          0          1          0   \n",
       "4      1        1        0          0          0          1          0   \n",
       "..   ...      ...      ...        ...        ...        ...        ...   \n",
       "95     0        1        0          0          1          0          0   \n",
       "96     0        1        0          0          1          0          0   \n",
       "97     0        1        0          0          1          0          0   \n",
       "98     0        1        0          0          1          0          0   \n",
       "99     0        1        0          0          1          0          0   \n",
       "\n",
       "    R_C_50_20  R_C_50_50  R_C_5_10  R_C_5_20  R_C_5_50  \n",
       "0           0          0         0         0         0  \n",
       "1           0          0         0         0         0  \n",
       "2           0          0         0         0         0  \n",
       "3           0          0         0         0         0  \n",
       "4           0          0         0         0         0  \n",
       "..        ...        ...       ...       ...       ...  \n",
       "95          0          0         0         0         0  \n",
       "96          0          0         0         0         0  \n",
       "97          0          0         0         0         0  \n",
       "98          0          0         0         0         0  \n",
       "99          0          0         0         0         0  \n",
       "\n",
       "[100 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train_std.csv\")\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "peaceful-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beginning-judgment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 22)\n",
      "(8000, 20) (8000, 1) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "debug = 1\n",
    "if debug:\n",
    "    df = df[:80*100]\n",
    "print(df.shape)\n",
    "X, Y = df.iloc[:, ~df.columns.isin(['pressure', 'id','Unnamed: 0'])], df.iloc[:, df.columns == 'pressure']\n",
    "Y = Y.to_numpy()#.reshape(-1, 80)\n",
    "X = X.to_numpy()#.reshape(-1, 80, X.shape[-1])\n",
    "print(X.shape, Y.shape, type(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-section",
   "metadata": {},
   "source": [
    "# 2. Classic Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-speed",
   "metadata": {},
   "source": [
    "> ## 2.1 Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "democratic-charleston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5600, 20) (5600, 1)\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into train and validation sets. here we follow a 70-30% split\n",
    "validation_size = 0.3\n",
    "seed = 123\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=validation_size, random_state=seed)\n",
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "binding-blade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate an array on algorithms to decide on baseline performance\n",
    "num_folds = 10 \n",
    "scoring = 'r2'\n",
    "#scoring = 'roc_auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "employed-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a series of candidate classifier algorithms with DEFAULT parameters. The target variable is highly\n",
    "# imbalanced so we will assign adaptive weights to the targets via the class_weight parameter\n",
    "\n",
    "models = [] \n",
    "models.append(('LR' , LinearRegression())) \n",
    "models.append(('Ridge' , Ridge())) \n",
    "models.append(('Lasso' , Lasso()))\n",
    "#models.append(('Poly' , GradientBoostingClassifier()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-graphic",
   "metadata": {},
   "source": [
    "> ## 2.2 Model Training and Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "favorite-matter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.395913 (0.025784)\n",
      "Ridge: 0.395923 (0.025753)\n",
      "Lasso: 0.312376 (0.018124)\n"
     ]
    }
   ],
   "source": [
    "results = [] \n",
    "names = [] \n",
    "\n",
    "for name, model in models: \n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True) \n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring) \n",
    "    results.append(cv_results) \n",
    "    names.append(name) \n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-saskatchewan",
   "metadata": {},
   "source": [
    "> ## 2.3 Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "light-intention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms \n",
    "sns.set(font_scale=1)\n",
    "fig = plt.figure() \n",
    "fig.suptitle(' Algorithm Comparison' ) \n",
    "ax = fig.add_subplot(111) \n",
    "plt.boxplot(results) \n",
    "ax.set_xticklabels(names)\n",
    "fig.savefig('ModelComparison.pdf',format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-sandwich",
   "metadata": {},
   "source": [
    "> ## 2.4 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "excess-astronomy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 19 candidates, totalling 190 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=10, random_state=123, shuffle=True),\n",
       "             estimator=RFE(estimator=LinearRegression()),\n",
       "             param_grid=[{'n_features_to_select': [1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
       "                                                   10, 11, 12, 13, 14, 15, 16,\n",
       "                                                   17, 18, 19]}],\n",
       "             return_train_score=True, scoring='r2', verbose=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GridSearch for linear regression\n",
    "folds = KFold(n_splits = 10, shuffle = True, random_state = 123)\n",
    "\n",
    "hyper_params = [{'n_features_to_select': list(range(1, 20))}]\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, Y_train)\n",
    "\n",
    "#RFE feature selection model\n",
    "rfe = RFE(lm)             \n",
    "\n",
    "model_cv = GridSearchCV(estimator = rfe, \n",
    "                        param_grid = hyper_params, \n",
    "                        scoring= 'r2', \n",
    "                        cv = folds, \n",
    "                        verbose = 1,\n",
    "                        return_train_score=True) \n",
    "\n",
    "model_cv.fit(X_train, Y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "attended-ribbon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_n_features_to_select</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.021</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>{'n_features_to_select': 1}</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.398</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.152</td>\n",
       "      <td>18</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.018</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "      <td>{'n_features_to_select': 2}</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.398</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.152</td>\n",
       "      <td>19</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3</td>\n",
       "      <td>{'n_features_to_select': 3}</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.398</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.150</td>\n",
       "      <td>17</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4</td>\n",
       "      <td>{'n_features_to_select': 4}</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.396</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.151</td>\n",
       "      <td>16</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.019</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.005</td>\n",
       "      <td>5</td>\n",
       "      <td>{'n_features_to_select': 5}</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.151</td>\n",
       "      <td>14</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.015</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6</td>\n",
       "      <td>{'n_features_to_select': 6}</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.151</td>\n",
       "      <td>15</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.015</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7</td>\n",
       "      <td>{'n_features_to_select': 7}</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.150</td>\n",
       "      <td>13</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.012</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.007</td>\n",
       "      <td>8</td>\n",
       "      <td>{'n_features_to_select': 8}</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.151</td>\n",
       "      <td>12</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.014</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9</td>\n",
       "      <td>{'n_features_to_select': 9}</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.151</td>\n",
       "      <td>11</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.012</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_features_to_select': 10}</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.119</td>\n",
       "      <td>10</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11</td>\n",
       "      <td>{'n_features_to_select': 11}</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.120</td>\n",
       "      <td>9</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12</td>\n",
       "      <td>{'n_features_to_select': 12}</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.026</td>\n",
       "      <td>8</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.011</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13</td>\n",
       "      <td>{'n_features_to_select': 13}</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.026</td>\n",
       "      <td>6</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>14</td>\n",
       "      <td>{'n_features_to_select': 14}</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.025</td>\n",
       "      <td>7</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>15</td>\n",
       "      <td>{'n_features_to_select': 15}</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.025</td>\n",
       "      <td>5</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>16</td>\n",
       "      <td>{'n_features_to_select': 16}</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.026</td>\n",
       "      <td>4</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>17</td>\n",
       "      <td>{'n_features_to_select': 17}</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.026</td>\n",
       "      <td>1</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>18</td>\n",
       "      <td>{'n_features_to_select': 18}</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.026</td>\n",
       "      <td>3</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>19</td>\n",
       "      <td>{'n_features_to_select': 19}</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.026</td>\n",
       "      <td>2</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0           0.021         0.007            0.000           0.000   \n",
       "1           0.018         0.005            0.000           0.000   \n",
       "2           0.020         0.007            0.000           0.000   \n",
       "3           0.016         0.005            0.000           0.000   \n",
       "4           0.019         0.006            0.002           0.005   \n",
       "5           0.015         0.002            0.000           0.000   \n",
       "6           0.015         0.003            0.000           0.000   \n",
       "7           0.012         0.006            0.005           0.007   \n",
       "8           0.014         0.005            0.000           0.000   \n",
       "9           0.012         0.006            0.000           0.000   \n",
       "10          0.009         0.008            0.000           0.000   \n",
       "11          0.009         0.008            0.000           0.000   \n",
       "12          0.011         0.007            0.000           0.000   \n",
       "13          0.009         0.008            0.000           0.000   \n",
       "14          0.008         0.008            0.000           0.000   \n",
       "15          0.008         0.008            0.000           0.000   \n",
       "16          0.008         0.008            0.000           0.000   \n",
       "17          0.005         0.007            0.000           0.000   \n",
       "18          0.005         0.007            0.000           0.000   \n",
       "\n",
       "   param_n_features_to_select                        params  \\\n",
       "0                           1   {'n_features_to_select': 1}   \n",
       "1                           2   {'n_features_to_select': 2}   \n",
       "2                           3   {'n_features_to_select': 3}   \n",
       "3                           4   {'n_features_to_select': 4}   \n",
       "4                           5   {'n_features_to_select': 5}   \n",
       "5                           6   {'n_features_to_select': 6}   \n",
       "6                           7   {'n_features_to_select': 7}   \n",
       "7                           8   {'n_features_to_select': 8}   \n",
       "8                           9   {'n_features_to_select': 9}   \n",
       "9                          10  {'n_features_to_select': 10}   \n",
       "10                         11  {'n_features_to_select': 11}   \n",
       "11                         12  {'n_features_to_select': 12}   \n",
       "12                         13  {'n_features_to_select': 13}   \n",
       "13                         14  {'n_features_to_select': 14}   \n",
       "14                         15  {'n_features_to_select': 15}   \n",
       "15                         16  {'n_features_to_select': 16}   \n",
       "16                         17  {'n_features_to_select': 17}   \n",
       "17                         18  {'n_features_to_select': 18}   \n",
       "18                         19  {'n_features_to_select': 19}   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0               0.350              0.405              0.359   \n",
       "1               0.350              0.405              0.359   \n",
       "2               0.354              0.409              0.358   \n",
       "3               0.358              0.418              0.371   \n",
       "4               0.358              0.421              0.375   \n",
       "5               0.363              0.420              0.373   \n",
       "6               0.369              0.414              0.374   \n",
       "7               0.372              0.417              0.371   \n",
       "8               0.371              0.419              0.372   \n",
       "9               0.371              0.418              0.374   \n",
       "10              0.371              0.418              0.374   \n",
       "11              0.372              0.418              0.374   \n",
       "12              0.372              0.418              0.375   \n",
       "13              0.372              0.417              0.375   \n",
       "14              0.372              0.417              0.375   \n",
       "15              0.372              0.417              0.375   \n",
       "16              0.372              0.417              0.375   \n",
       "17              0.372              0.417              0.375   \n",
       "18              0.372              0.417              0.375   \n",
       "\n",
       "    split3_test_score  split4_test_score  split5_test_score  \\\n",
       "0               0.364              0.325              0.006   \n",
       "1               0.364              0.325              0.011   \n",
       "2               0.362              0.331              0.019   \n",
       "3               0.369              0.342              0.020   \n",
       "4               0.372              0.350              0.020   \n",
       "5               0.364              0.352              0.020   \n",
       "6               0.366              0.357              0.020   \n",
       "7               0.364              0.362              0.025   \n",
       "8               0.363              0.361              0.025   \n",
       "9               0.363              0.361              0.425   \n",
       "10              0.363              0.362              0.425   \n",
       "11              0.362              0.362              0.433   \n",
       "12              0.362              0.362              0.433   \n",
       "13              0.362              0.362              0.433   \n",
       "14              0.362              0.362              0.433   \n",
       "15              0.362              0.362              0.433   \n",
       "16              0.362              0.362              0.433   \n",
       "17              0.362              0.362              0.433   \n",
       "18              0.362              0.362              0.433   \n",
       "\n",
       "    split6_test_score  split7_test_score  split8_test_score  \\\n",
       "0               0.375              0.398             -0.005   \n",
       "1               0.375              0.398             -0.010   \n",
       "2               0.372              0.398             -0.004   \n",
       "3               0.368              0.396             -0.005   \n",
       "4               0.371              0.401              0.002   \n",
       "5               0.374              0.402              0.002   \n",
       "6               0.380              0.402              0.006   \n",
       "7               0.386              0.402              0.001   \n",
       "8               0.386              0.402              0.001   \n",
       "9               0.386              0.402              0.000   \n",
       "10              0.386              0.403              0.000   \n",
       "11              0.386              0.403              0.420   \n",
       "12              0.386              0.404              0.420   \n",
       "13              0.387              0.404              0.420   \n",
       "14              0.387              0.404              0.420   \n",
       "15              0.387              0.405              0.420   \n",
       "16              0.387              0.405              0.425   \n",
       "17              0.387              0.405              0.423   \n",
       "18              0.387              0.405              0.423   \n",
       "\n",
       "    split9_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
       "0               0.424            0.300           0.152               18   \n",
       "1               0.424            0.300           0.152               19   \n",
       "2               0.423            0.302           0.150               17   \n",
       "3               0.422            0.306           0.151               16   \n",
       "4               0.429            0.310           0.151               14   \n",
       "5               0.427            0.310           0.151               15   \n",
       "6               0.419            0.311           0.150               13   \n",
       "7               0.421            0.312           0.151               12   \n",
       "8               0.420            0.312           0.151               11   \n",
       "9               0.420            0.352           0.119               10   \n",
       "10              0.420            0.352           0.120                9   \n",
       "11              0.420            0.395           0.026                8   \n",
       "12              0.421            0.395           0.026                6   \n",
       "13              0.421            0.395           0.025                7   \n",
       "14              0.421            0.395           0.025                5   \n",
       "15              0.421            0.395           0.026                4   \n",
       "16              0.421            0.396           0.026                1   \n",
       "17              0.421            0.396           0.026                3   \n",
       "18              0.421            0.396           0.026                2   \n",
       "\n",
       "    split0_train_score  split1_train_score  split2_train_score  \\\n",
       "0                0.387               0.381               0.386   \n",
       "1                0.387               0.381               0.386   \n",
       "2                0.388               0.382               0.388   \n",
       "3                0.393               0.387               0.392   \n",
       "4                0.394               0.391               0.396   \n",
       "5                0.399               0.392               0.398   \n",
       "6                0.401               0.395               0.400   \n",
       "7                0.402               0.396               0.401   \n",
       "8                0.402               0.397               0.401   \n",
       "9                0.402               0.397               0.402   \n",
       "10               0.402               0.397               0.402   \n",
       "11               0.402               0.397               0.402   \n",
       "12               0.402               0.397               0.402   \n",
       "13               0.402               0.397               0.402   \n",
       "14               0.402               0.397               0.402   \n",
       "15               0.402               0.397               0.402   \n",
       "16               0.402               0.397               0.402   \n",
       "17               0.402               0.397               0.402   \n",
       "18               0.402               0.397               0.402   \n",
       "\n",
       "    split3_train_score  split4_train_score  split5_train_score  \\\n",
       "0                0.385               0.389               0.001   \n",
       "1                0.385               0.389               0.002   \n",
       "2                0.387               0.390               0.007   \n",
       "3                0.392               0.395               0.009   \n",
       "4                0.396               0.397               0.009   \n",
       "5                0.401               0.399               0.009   \n",
       "6                0.402               0.401               0.009   \n",
       "7                0.403               0.403               0.013   \n",
       "8                0.403               0.403               0.013   \n",
       "9                0.403               0.403               0.392   \n",
       "10               0.403               0.403               0.392   \n",
       "11               0.403               0.403               0.395   \n",
       "12               0.403               0.403               0.395   \n",
       "13               0.403               0.403               0.395   \n",
       "14               0.404               0.403               0.395   \n",
       "15               0.404               0.403               0.395   \n",
       "16               0.404               0.403               0.395   \n",
       "17               0.404               0.403               0.395   \n",
       "18               0.404               0.403               0.395   \n",
       "\n",
       "    split6_train_score  split7_train_score  split8_train_score  \\\n",
       "0                0.384               0.380               0.001   \n",
       "1                0.384               0.380               0.004   \n",
       "2                0.386               0.382               0.009   \n",
       "3                0.392               0.388               0.011   \n",
       "4                0.395               0.392               0.012   \n",
       "5                0.396               0.395               0.012   \n",
       "6                0.399               0.396               0.011   \n",
       "7                0.400               0.397               0.015   \n",
       "8                0.400               0.397               0.015   \n",
       "9                0.400               0.398               0.015   \n",
       "10               0.400               0.398               0.015   \n",
       "11               0.400               0.398               0.394   \n",
       "12               0.400               0.398               0.394   \n",
       "13               0.400               0.398               0.394   \n",
       "14               0.400               0.398               0.393   \n",
       "15               0.400               0.398               0.394   \n",
       "16               0.400               0.398               0.396   \n",
       "17               0.400               0.398               0.397   \n",
       "18               0.400               0.398               0.397   \n",
       "\n",
       "    split9_train_score  mean_train_score  std_train_score  \n",
       "0                0.379             0.307            0.153  \n",
       "1                0.379             0.308            0.152  \n",
       "2                0.381             0.310            0.151  \n",
       "3                0.386             0.314            0.152  \n",
       "4                0.389             0.317            0.153  \n",
       "5                0.391             0.319            0.154  \n",
       "6                0.395             0.321            0.155  \n",
       "7                0.396             0.323            0.154  \n",
       "8                0.397             0.323            0.155  \n",
       "9                0.397             0.361            0.115  \n",
       "10               0.397             0.361            0.115  \n",
       "11               0.397             0.399            0.003  \n",
       "12               0.397             0.399            0.003  \n",
       "13               0.397             0.399            0.003  \n",
       "14               0.397             0.399            0.003  \n",
       "15               0.397             0.399            0.003  \n",
       "16               0.397             0.399            0.003  \n",
       "17               0.397             0.400            0.003  \n",
       "18               0.397             0.400            0.003  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-respect",
   "metadata": {},
   "source": [
    "plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "alert-mailing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x19203bce220>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting cv results\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_test_score\"])\n",
    "plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_train_score\"])\n",
    "plt.xlabel('number of features')\n",
    "plt.ylabel('r-squared')\n",
    "plt.title(\"Optimal Number of Features\")\n",
    "plt.legend(['test score', 'train score'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-zambia",
   "metadata": {},
   "source": [
    "> ## 2.5 Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "opposite-providence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41945973849316864\n"
     ]
    }
   ],
   "source": [
    "# final model\n",
    "n_features_optimal = 12\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, Y_train)\n",
    "\n",
    "rfe = RFE(lm, n_features_to_select=n_features_optimal)             \n",
    "rfe = rfe.fit(X_train, Y_train)\n",
    "\n",
    "# predict prices of X_test\n",
    "y_pred = lm.predict(X_test)\n",
    "r2 = r2_score(Y_test, y_pred)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-chair",
   "metadata": {},
   "source": [
    "# 3. Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "retired-lounge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 80, 20) (100, 80) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "if debug:\n",
    "    df_copy = df[:80*2000]\n",
    "#df_copy.info()\n",
    "X, Y = df_copy.iloc[:, ~df.columns.isin(['pressure', 'id','Unnamed: 0'])], df_copy.iloc[:, df.columns == 'pressure']\n",
    "Y = Y.to_numpy().reshape(-1, 80)\n",
    "\n",
    "RS = RobustScaler()\n",
    "X = RS.fit_transform(X)\n",
    "\n",
    "X = X.reshape(-1, 80, X.shape[-1])\n",
    "print(X.shape, Y.shape, type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fitted-finding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- > Fold 1 < ---------------\n",
      "Epoch 1/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 11.2598\n",
      "Epoch 00001: val_loss improved from inf to 8.23467, saving model to folds0.hdf5\n",
      "1/1 [==============================] - 6s 6s/step - loss: 11.2598 - val_loss: 8.2347\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 7.9078\n",
      "Epoch 00002: val_loss improved from 8.23467 to 5.83113, saving model to folds0.hdf5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 7.9078 - val_loss: 5.8311\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 5.4022\n",
      "Epoch 00003: val_loss improved from 5.83113 to 5.42050, saving model to folds0.hdf5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 5.4022 - val_loss: 5.4205\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.9882\n",
      "Epoch 00004: val_loss did not improve from 5.42050\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.9882 - val_loss: 5.6813\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 5.2920\n",
      "Epoch 00005: val_loss improved from 5.42050 to 5.40545, saving model to folds0.hdf5\n",
      "1/1 [==============================] - 4s 4s/step - loss: 5.2920 - val_loss: 5.4055\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.9730\n",
      "Epoch 00006: val_loss did not improve from 5.40545\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.9730 - val_loss: 5.5728\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 5.1392\n",
      "Epoch 00007: val_loss did not improve from 5.40545\n",
      "1/1 [==============================] - 3s 3s/step - loss: 5.1392 - val_loss: 5.5543\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 5.1207\n",
      "Epoch 00008: val_loss did not improve from 5.40545\n",
      "1/1 [==============================] - 3s 3s/step - loss: 5.1207 - val_loss: 5.4195\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.9877\n",
      "Epoch 00009: val_loss improved from 5.40545 to 5.32111, saving model to folds0.hdf5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.9877 - val_loss: 5.3211\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.9180\n",
      "Epoch 00010: val_loss did not improve from 5.32111\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.9180 - val_loss: 5.3688\n",
      "Epoch 11/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.9792\n",
      "Epoch 00011: val_loss did not improve from 5.32111\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.9792 - val_loss: 5.3391\n",
      "Epoch 12/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.9476\n",
      "Epoch 00012: val_loss improved from 5.32111 to 5.29024, saving model to folds0.hdf5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.9476 - val_loss: 5.2902\n",
      "Epoch 13/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.8845\n",
      "Epoch 00013: val_loss did not improve from 5.29024\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.8845 - val_loss: 5.2985\n",
      "Epoch 14/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.8760\n",
      "Epoch 00014: val_loss improved from 5.29024 to 5.28250, saving model to folds0.hdf5\n",
      "1/1 [==============================] - 4s 4s/step - loss: 4.8760 - val_loss: 5.2825\n",
      "Epoch 15/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.8564\n",
      "Epoch 00015: val_loss improved from 5.28250 to 5.19857, saving model to folds0.hdf5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.8564 - val_loss: 5.1986\n",
      "Epoch 16/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.7791\n",
      "Epoch 00016: val_loss improved from 5.19857 to 5.09276, saving model to folds0.hdf5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.7791 - val_loss: 5.0928\n",
      "Epoch 17/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.6803\n",
      "Epoch 00017: val_loss did not improve from 5.09276\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.6803 - val_loss: 5.4735\n",
      "Epoch 18/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 5.0552\n",
      "Epoch 00018: val_loss did not improve from 5.09276\n",
      "1/1 [==============================] - 3s 3s/step - loss: 5.0552 - val_loss: 5.1654\n",
      "Epoch 19/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.7637\n",
      "Epoch 00019: val_loss did not improve from 5.09276\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.7637 - val_loss: 5.1446\n",
      "Epoch 20/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.7206\n",
      "Epoch 00020: val_loss did not improve from 5.09276\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.7206 - val_loss: 5.1969\n",
      "Epoch 21/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.7688\n",
      "Epoch 00021: val_loss improved from 5.09276 to 4.97306, saving model to folds0.hdf5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.7688 - val_loss: 4.9731\n",
      "Epoch 22/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.5531\n",
      "Epoch 00022: val_loss improved from 4.97306 to 4.64278, saving model to folds0.hdf5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.5531 - val_loss: 4.6428\n",
      "Epoch 23/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.2389\n",
      "Epoch 00023: val_loss did not improve from 4.64278\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.2389 - val_loss: 5.3231\n",
      "Epoch 24/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.9267\n",
      "Epoch 00024: val_loss did not improve from 4.64278\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.9267 - val_loss: 5.5222\n",
      "Epoch 25/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 5.1235\n",
      "Epoch 00025: val_loss did not improve from 4.64278\n",
      "1/1 [==============================] - 3s 3s/step - loss: 5.1235 - val_loss: 4.7175\n",
      "Epoch 26/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.3012\n",
      "Epoch 00026: val_loss did not improve from 4.64278\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.3012 - val_loss: 5.1540\n",
      "Epoch 27/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.7816\n",
      "Epoch 00027: val_loss did not improve from 4.64278\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.7816 - val_loss: 4.9670\n",
      "Epoch 28/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.6077\n",
      "Epoch 00028: val_loss improved from 4.64278 to 4.63018, saving model to folds0.hdf5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.6077 - val_loss: 4.6302\n",
      "Epoch 29/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.2284\n",
      "Epoch 00029: val_loss did not improve from 4.63018\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.2284 - val_loss: 4.6866\n",
      "Epoch 30/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.2817\n",
      "Epoch 00030: val_loss improved from 4.63018 to 4.43634, saving model to folds0.hdf5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.2817 - val_loss: 4.4363\n",
      "Epoch 31/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.0406\n",
      "Epoch 00031: val_loss did not improve from 4.43634\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.0406 - val_loss: 5.5872\n",
      "Epoch 32/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 5.2013\n",
      "Epoch 00032: val_loss did not improve from 4.43634\n",
      "1/1 [==============================] - 3s 3s/step - loss: 5.2013 - val_loss: 4.5373\n",
      "Epoch 33/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.1305\n",
      "Epoch 00033: val_loss did not improve from 4.43634\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.1305 - val_loss: 4.8125\n",
      "Epoch 34/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.3999\n",
      "Epoch 00034: val_loss did not improve from 4.43634\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.3999 - val_loss: 4.8290\n",
      "Epoch 35/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.4247\n",
      "Epoch 00035: val_loss did not improve from 4.43634\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.4247 - val_loss: 4.8448\n",
      "Epoch 36/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.4373\n",
      "Epoch 00036: val_loss did not improve from 4.43634\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.4373 - val_loss: 4.8186\n",
      "Epoch 37/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.4190\n",
      "Epoch 00037: val_loss did not improve from 4.43634\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.4190 - val_loss: 4.6304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.2483\n",
      "Epoch 00038: val_loss improved from 4.43634 to 4.35438, saving model to folds0.hdf5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.2483 - val_loss: 4.3544\n",
      "Epoch 39/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.9870\n",
      "Epoch 00039: val_loss improved from 4.35438 to 4.03564, saving model to folds0.hdf5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 3.9870 - val_loss: 4.0356\n",
      "Epoch 40/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.6874\n",
      "Epoch 00040: val_loss did not improve from 4.03564\n",
      "1/1 [==============================] - 3s 3s/step - loss: 3.6874 - val_loss: 5.0588\n",
      "Epoch 41/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 5.0434\n",
      "Epoch 00041: val_loss improved from 4.03564 to 3.93556, saving model to folds0.hdf5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 5.0434 - val_loss: 3.9356\n",
      "Epoch 42/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.6729\n",
      "Epoch 00042: val_loss did not improve from 3.93556\n",
      "1/1 [==============================] - 3s 3s/step - loss: 3.6729 - val_loss: 4.0375\n",
      "Epoch 43/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.6616\n",
      "Epoch 00043: val_loss did not improve from 3.93556\n",
      "1/1 [==============================] - 3s 3s/step - loss: 3.6616 - val_loss: 4.1124\n",
      "Epoch 44/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.7434\n",
      "Epoch 00044: val_loss improved from 3.93556 to 3.90545, saving model to folds0.hdf5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 3.7434 - val_loss: 3.9055\n",
      "Epoch 45/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.5311\n",
      "Epoch 00045: val_loss improved from 3.90545 to 3.87423, saving model to folds0.hdf5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 3.5311 - val_loss: 3.8742\n",
      "Epoch 46/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.6277\n",
      "Epoch 00046: val_loss improved from 3.87423 to 3.77898, saving model to folds0.hdf5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 3.6277 - val_loss: 3.7790\n",
      "Epoch 47/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.3345\n",
      "Epoch 00047: val_loss improved from 3.77898 to 3.56097, saving model to folds0.hdf5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 3.3345 - val_loss: 3.5610\n",
      "Epoch 48/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.1476\n",
      "Epoch 00048: val_loss improved from 3.56097 to 3.43629, saving model to folds0.hdf5\n",
      "1/1 [==============================] - 4s 4s/step - loss: 3.1476 - val_loss: 3.4363\n",
      "Epoch 49/300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-12bfc15a0d94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m     )\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[1;31m#model.save(f'Fold{fold+1} RNN Weights')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mtest_preds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCH = 300\n",
    "BATCH_SIZE = 1024\n",
    "NUM_FOLDS = 2\n",
    "\n",
    "kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=103)\n",
    "test_preds = []\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X, Y)):\n",
    "    print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n",
    "    X_train, X_valid = X[train_idx], X[test_idx]\n",
    "    y_train, y_valid = Y[train_idx], Y[test_idx]\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Input(shape=X.shape[-2:]),\n",
    "        keras.layers.Bidirectional(keras.layers.LSTM(1024,return_sequences=True)),\n",
    "        keras.layers.Bidirectional(keras.layers.LSTM(512, return_sequences=True)),\n",
    "        keras.layers.Bidirectional(keras.layers.LSTM(256, return_sequences=True)),\n",
    "        keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True)),\n",
    "#             keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True)),\n",
    "        keras.layers.Dense(128, activation='selu'),\n",
    "#             keras.layers.Dropout(0.1),\n",
    "        keras.layers.Dense(1),\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mae\")\n",
    "\n",
    "#         scheduler = ExponentialDecay(1e-3, 40*((len(train)*0.8)/BATCH_SIZE), 1e-5)\n",
    "#         lr = LearningRateScheduler(scheduler, verbose=1)\n",
    "    lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n",
    "#         lr = WarmupExponentialDecay(lr_base=1e-3, decay=1e-5, warmup_epochs=30)\n",
    "    es = EarlyStopping(monitor=\"val_loss\", patience=60, verbose=1, mode=\"min\", restore_best_weights=True)\n",
    "\n",
    "    checkpoint_filepath = f\"folds{fold}.hdf5\"\n",
    "    sv = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=False, mode='auto', save_freq='epoch',\n",
    "        options=None\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=EPOCH, batch_size=BATCH_SIZE, callbacks=[lr, es, sv])\n",
    "    #model.save(f'Fold{fold+1} RNN Weights')\n",
    "    test_preds.append(model.predict(test).squeeze().reshape(-1, 1).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-boundary",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 58.786749,
   "end_time": "2022-01-04T05:59:07.540755",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-04T05:58:08.754006",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
